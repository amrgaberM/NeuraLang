{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Mathematics: Step-by-Step Guide\n",
    "\n",
    "This notebook provides a detailed explanation of the self-attention mechanism, breaking down the mathematics behind Query (Q), Key (K), and Value (V) matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c810cb64f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Input\n",
    "\n",
    "Let's start with a simple sentence and create token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings (X):\n",
      "tensor([[1., 0., 1., 0.],\n",
      "        [0., 1., 0., 1.],\n",
      "        [1., 1., 0., 0.]])\n",
      "\n",
      "Shape: torch.Size([3, 4]) (seq_len=3, d_model=4)\n"
     ]
    }
   ],
   "source": [
    "# Simple example: \"The cat sat\"\n",
    "seq_len = 3\n",
    "d_model = 4  # Small dimension for clarity\n",
    "\n",
    "# Create simple token embeddings (normally from embedding layer)\n",
    "X = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],  # \"The\"\n",
    "    [0.0, 1.0, 0.0, 1.0],  # \"cat\"\n",
    "    [1.0, 1.0, 0.0, 0.0]   # \"sat\"\n",
    "])\n",
    "\n",
    "print(\"Input Embeddings (X):\")\n",
    "print(X)\n",
    "print(f\"\\nShape: {X.shape} (seq_len={seq_len}, d_model={d_model})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Query, Key, and Value Matrices\n",
    "\n",
    "The core of attention is projecting the input into three different spaces:\n",
    "- **Query (Q)**: What am I looking for?\n",
    "- **Key (K)**: What do I contain?\n",
    "- **Value (V)**: What information do I have to offer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weight matrices (in practice, these are learned)\n",
    "W_q = torch.randn(d_model, d_model) * 0.1\n",
    "W_k = torch.randn(d_model, d_model) * 0.1\n",
    "W_v = torch.randn(d_model, d_model) * 0.1\n",
    "\n",
    "print(\"Weight Matrices:\")\n",
    "print(f\"W_q shape: {W_q.shape}\")\n",
    "print(f\"W_k shape: {W_k.shape}\")\n",
    "print(f\"W_v shape: {W_v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Q, K, V by matrix multiplication\n",
    "Q = X @ W_q  # Query: what each token is looking for\n",
    "K = X @ W_k  # Key: what each token represents\n",
    "V = X @ W_v  # Value: what information each token carries\n",
    "\n",
    "print(\"Query Matrix (Q):\")\n",
    "print(Q)\n",
    "print(f\"\\nKey Matrix (K):\")\n",
    "print(K)\n",
    "print(f\"\\nValue Matrix (V):\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing Attention Scores\n",
    "\n",
    "Calculate how much each token should attend to every other token.\n",
    "\n",
    "**Formula**: $\\text{scores} = \\frac{Q \\cdot K^T}{\\sqrt{d_k}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute raw attention scores (Q @ K^T)\n",
    "d_k = d_model  # In our case, they're the same\n",
    "raw_scores = Q @ K.transpose(-2, -1)\n",
    "\n",
    "print(\"Raw Attention Scores (Q @ K^T):\")\n",
    "print(raw_scores)\n",
    "print(f\"\\nShape: {raw_scores.shape}\")\n",
    "print(\"\\nInterpretation: Element (i,j) = how much token i attends to token j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale by sqrt(d_k) to prevent large values\n",
    "import math\n",
    "\n",
    "scaled_scores = raw_scores / math.sqrt(d_k)\n",
    "\n",
    "print(f\"Scaled Attention Scores (divided by sqrt({d_k}) = {math.sqrt(d_k):.2f}):\")\n",
    "print(scaled_scores)\n",
    "print(\"\\nScaling prevents gradient vanishing in softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applying Softmax to Get Attention Weights\n",
    "\n",
    "Convert scores to probabilities that sum to 1 for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply softmax to each row\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)\n",
    "\n",
    "print(\"Attention Weights (after softmax):\")\n",
    "print(attention_weights)\n",
    "print(f\"\\nRow sums (should be 1.0): {attention_weights.sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attention_weights.numpy(), \n",
    "            annot=True, \n",
    "            fmt='.3f', \n",
    "            cmap='YlOrRd',\n",
    "            xticklabels=['The', 'cat', 'sat'],\n",
    "            yticklabels=['The', 'cat', 'sat'],\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Attention Weight Matrix\\n(Row i shows how token i attends to all tokens)', fontsize=14)\n",
    "plt.xlabel('Keys (attending TO)', fontsize=12)\n",
    "plt.ylabel('Queries (attending FROM)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computing the Final Attention Output\n",
    "\n",
    "Weighted sum of values using attention weights.\n",
    "\n",
    "**Formula**: $\\text{output} = \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply attention weights with values\n",
    "attention_output = attention_weights @ V\n",
    "\n",
    "print(\"Attention Output:\")\n",
    "print(attention_output)\n",
    "print(f\"\\nShape: {attention_output.shape}\")\n",
    "print(\"\\nEach row is a weighted combination of all value vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Self-Attention Function\n",
    "\n",
    "Let's put it all together in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(X, W_q, W_k, W_v, mask=None):\n",
    "    \"\"\"\n",
    "    Complete self-attention mechanism\n",
    "    \n",
    "    Args:\n",
    "        X: Input embeddings (seq_len, d_model)\n",
    "        W_q, W_k, W_v: Weight matrices\n",
    "        mask: Optional attention mask\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output\n",
    "        attention_weights: Attention distribution\n",
    "    \"\"\"\n",
    "    # Step 1: Project to Q, K, V\n",
    "    Q = X @ W_q\n",
    "    K = X @ W_k\n",
    "    V = X @ W_v\n",
    "    \n",
    "    # Step 2: Compute scaled attention scores\n",
    "    d_k = Q.size(-1)\n",
    "    scores = (Q @ K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Step 4: Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 5: Compute weighted sum of values\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test the function\n",
    "output, weights = self_attention(X, W_q, W_k, W_v)\n",
    "print(\"Self-Attention Output:\")\n",
    "print(output)\n",
    "print(f\"\\nMatches previous computation: {torch.allclose(output, attention_output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding Masking (Optional)\n",
    "\n",
    "Masks prevent attention to certain positions (e.g., future tokens in autoregressive models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a causal mask (lower triangular)\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "\n",
    "print(\"Causal Mask (prevents attending to future tokens):\")\n",
    "print(causal_mask)\n",
    "\n",
    "# Apply masked attention\n",
    "masked_output, masked_weights = self_attention(X, W_q, W_k, W_v, mask=causal_mask)\n",
    "\n",
    "print(\"\\nMasked Attention Weights:\")\n",
    "print(masked_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize masked attention\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original attention\n",
    "sns.heatmap(attention_weights.numpy(), \n",
    "            annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=['The', 'cat', 'sat'],\n",
    "            yticklabels=['The', 'cat', 'sat'],\n",
    "            ax=axes[0])\n",
    "axes[0].set_title('Standard Self-Attention', fontsize=12)\n",
    "\n",
    "# Masked attention\n",
    "sns.heatmap(masked_weights.numpy(), \n",
    "            annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=['The', 'cat', 'sat'],\n",
    "            yticklabels=['The', 'cat', 'sat'],\n",
    "            ax=axes[1])\n",
    "axes[1].set_title('Causal (Masked) Self-Attention', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Intuitions\n",
    "\n",
    "### What did we learn?\n",
    "\n",
    "1. **Query (Q)**: Represents what each token is \"searching for\"\n",
    "2. **Key (K)**: Represents what each token \"offers\" or \"contains\"\n",
    "3. **Value (V)**: The actual information to be aggregated\n",
    "\n",
    "### The Attention Mechanism:\n",
    "- **Scores**: Dot product between queries and keys measures relevance\n",
    "- **Scaling**: Dividing by âˆšd_k stabilizes gradients\n",
    "- **Softmax**: Converts scores to a probability distribution\n",
    "- **Output**: Weighted combination of values based on attention\n",
    "\n",
    "### Why is this powerful?\n",
    "- **Dynamic**: Each token can attend to any other token\n",
    "- **Contextual**: Representations depend on the entire sequence\n",
    "- **Parallelizable**: All computations can be done simultaneously\n",
    "- **Learnable**: Q, K, V projections are learned from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multi-Head Attention Preview\n",
    "\n",
    "In practice, we use multiple attention heads to capture different types of relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate 2 attention heads\n",
    "num_heads = 2\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "print(f\"Original dimension: {d_model}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Dimension per head: {d_k}\")\n",
    "print(\"\\nEach head learns different attention patterns!\")\n",
    "print(\"- Head 1 might focus on syntax (e.g., subject-verb agreement)\")\n",
    "print(\"- Head 2 might focus on semantics (e.g., word meanings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Self-Attention Formula**:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q = XW_q$ (Queries)\n",
    "- $K = XW_k$ (Keys)\n",
    "- $V = XW_v$ (Values)\n",
    "\n",
    "This mechanism allows each position in a sequence to attend to all positions, creating rich contextual representations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
