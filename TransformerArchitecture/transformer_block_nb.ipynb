{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder Block: Complete Implementation\n",
    "\n",
    "This notebook implements and explains a complete transformer encoder block with:\n",
    "- Multi-head self-attention\n",
    "- Position-wise feed-forward network\n",
    "- Layer normalization\n",
    "- Residual connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import from building blocks\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from building_blocks import MultiHeadAttention, PositionWiseFFN, TransformerBlock, PositionalEncoding\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Block Components\n",
    "\n",
    "Let's review each component of a transformer block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Multi-Head Attention\n",
    "\n",
    "Multiple attention heads capture different types of relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "# Create sample input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Initialize multi-head attention\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "attention_output = mha(x, x, x)\n",
    "\n",
    "print(\"Multi-Head Attention:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {attention_output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in mha.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Position-Wise Feed-Forward Network\n",
    "\n",
    "Applied independently to each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-forward network\n",
    "d_ff = 2048  # Typically 4 * d_model\n",
    "ffn = PositionWiseFFN(d_model, d_ff)\n",
    "ffn_output = ffn(x)\n",
    "\n",
    "print(\"Position-Wise Feed-Forward Network:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Hidden dimension: {d_ff}\")\n",
    "print(f\"Output shape: {ffn_output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in ffn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Layer Normalization\n",
    "\n",
    "Normalizes across the feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer normalization\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "# Compare statistics before and after\n",
    "print(\"Before Layer Norm:\")\n",
    "print(f\"Mean: {x.mean():.4f}\")\n",
    "print(f\"Std: {x.std():.4f}\")\n",
    "\n",
    "normalized = layer_norm(x)\n",
    "print(\"\\nAfter Layer Norm:\")\n",
    "print(f\"Mean: {normalized.mean():.4f}\")\n",
    "print(f\"Std: {normalized.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Transformer Block\n",
    "\n",
    "Combining all components with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transformer block\n",
    "transformer_block = TransformerBlock(d_model, num_heads, d_ff, dropout=0.1)\n",
    "\n",
    "print(\"Transformer Block Architecture:\")\n",
    "print(transformer_block)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in transformer_block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "output = transformer_block(x)\n",
    "\n",
    "print(\"Transformer Block Forward Pass:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nShape preserved: {x.shape == output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Data Flow\n",
    "\n",
    "Let's trace how data flows through the transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track intermediate outputs\n",
    "def transformer_block_detailed(x, block):\n",
    "    \"\"\"\n",
    "    Detailed forward pass with intermediate outputs\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "    outputs['input'] = x.clone()\n",
    "    \n",
    "    # Multi-head attention\n",
    "    attn_out = block.attention(x, x, x)\n",
    "    outputs['attention'] = attn_out.clone()\n",
    "    \n",
    "    # First residual connection\n",
    "    x = x + block.dropout1(attn_out)\n",
    "    outputs['after_residual_1'] = x.clone()\n",
    "    \n",
    "    # First layer norm\n",
    "    x = block.norm1(x)\n",
    "    outputs['after_norm_1'] = x.clone()\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn_out = block.ffn(x)\n",
    "    outputs['ffn'] = ffn_out.clone()\n",
    "    \n",
    "    # Second residual connection\n",
    "    x = x + block.dropout2(ffn_out)\n",
    "    outputs['after_residual_2'] = x.clone()\n",
    "    \n",
    "    # Second layer norm\n",
    "    x = block.norm2(x)\n",
    "    outputs['output'] = x.clone()\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Run detailed forward pass\n",
    "sample_input = torch.randn(1, 4, d_model)\n",
    "intermediates = transformer_block_detailed(sample_input, transformer_block)\n",
    "\n",
    "# Display statistics at each stage\n",
    "print(\"Data Flow Statistics:\\n\")\n",
    "for name, tensor in intermediates.items():\n",
    "    print(f\"{name:20s} | Mean: {tensor.mean():7.4f} | Std: {tensor.std():7.4f} | Shape: {tuple(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Effect of Residual Connections\n",
    "\n",
    "Residual connections help with gradient flow and training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with and without residuals\n",
    "class TransformerBlockNoResidual(nn.Module):\n",
    "    \"\"\"Transformer block WITHOUT residual connections\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = PositionWiseFFN(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # No residual connections!\n",
    "        x = self.norm1(self.attention(x, x, x, mask))\n",
    "        x = self.norm2(self.ffn(x))\n",
    "        return x\n",
    "\n",
    "# Stack multiple blocks\n",
    "num_layers = 6\n",
    "input_tensor = torch.randn(1, 4, d_model)\n",
    "\n",
    "# With residuals\n",
    "blocks_with = nn.ModuleList([TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "x_with = input_tensor.clone()\n",
    "norms_with = []\n",
    "for block in blocks_with:\n",
    "    x_with = block(x_with)\n",
    "    norms_with.append(x_with.norm().item())\n",
    "\n",
    "# Without residuals\n",
    "blocks_without = nn.ModuleList([TransformerBlockNoResidual(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "x_without = input_tensor.clone()\n",
    "norms_without = []\n",
    "for block in blocks_without:\n",
    "    x_without = block(x_without)\n",
    "    norms_without.append(x_without.norm().item())\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_layers + 1), norms_with, marker='o', label='With Residuals', linewidth=2)\n",
    "plt.plot(range(1, num_layers + 1), norms_without, marker='s', label='Without Residuals', linewidth=2)\n",
    "plt.xlabel('Layer', fontsize=12)\n",
    "plt.ylabel('Output Norm', fontsize=12)\n",
    "plt.title('Effect of Residual Connections on Deep Networks', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final norm with residuals: {norms_with[-1]:.4f}\")\n",
    "print(f\"Final norm without residuals: {norms_without[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Positional Encoding\n",
    "\n",
    "Since attention has no notion of position, we add positional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create positional encoding\n",
    "pos_encoding = PositionalEncoding(d_model, max_len=100)\n",
    "\n",
    "# Visualize positional encoding\n",
    "pe_matrix = pos_encoding.pe.squeeze(0)[:50, :].numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pe_matrix.T, cmap='RdBu', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Position', fontsize=12)\n",
    "plt.ylabel('Dimension', fontsize=12)\n",
    "plt.title('Sinusoidal Positional Encoding\\n(First 50 positions)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply positional encoding to embeddings\n",
    "embeddings = torch.randn(batch_size, seq_len, d_model)\n",
    "embeddings_with_pos = pos_encoding(embeddings)\n",
    "\n",
    "print(\"Positional Encoding:\")\n",
    "print(f\"Input embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Output shape: {embeddings_with_pos.shape}\")\n",
    "print(f\"\\nPositional encoding added to each position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Transformer Encoder\n",
    "\n",
    "Stack multiple transformer blocks to create a full encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices (batch, seq_len)\n",
    "            mask: Optional attention mask\n",
    "        \"\"\"\n",
    "        # Embed tokens and add positional encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)  # Scale by sqrt(d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize encoder\n",
    "vocab_size = 10000\n",
    "num_layers = 6\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers)\n",
    "\n",
    "print(\"Complete Transformer Encoder:\")\n",
    "print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"Model dimension: {d_model}\")\n",
    "print(f\"Number of layers: {num_layers}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the encoder\n",
    "sample_tokens = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "encoder_output = encoder(sample_tokens)\n",
    "\n",
    "print(\"\\nEncoder Forward Pass:\")\n",
    "print(f\"Input tokens shape: {sample_tokens.shape}\")\n",
    "print(f\"Output embeddings shape: {encoder_output.shape}\")\n",
    "print(f\"\\nEach token now has a contextualized representation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Attention Pattern Analysis\n",
    "\n",
    "Let's visualize what the attention heads are learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention weights from a specific layer\n",
    "def get_attention_weights(model, x, layer_idx=0):\n",
    "    \"\"\"\n",
    "    Extract attention weights from a specific layer\n",
    "    \"\"\"\n",
    "    # Embed and add positional encoding\n",
    "    x = model.embedding(x) * math.sqrt(model.d_model)\n",
    "    x = model.pos_encoding(x)\n",
    "    \n",
    "    # Get to specified layer\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if i == layer_idx:\n",
    "            # Extract attention weights\n",
    "            Q = layer.attention.W_q(x)\n",
    "            K = layer.attention.W_k(x)\n",
    "            \n",
    "            batch_size = x.size(0)\n",
    "            Q = Q.view(batch_size, -1, layer.attention.num_heads, layer.attention.d_k).transpose(1, 2)\n",
    "            K = K.view(batch_size, -1, layer.attention.num_heads, layer.attention.d_k).transpose(1, 2)\n",
    "            \n",
    "            scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(layer.attention.d_k)\n",
    "            attention_weights = F.softmax(scores, dim=-1)\n",
    "            \n",
    "            return attention_weights\n",
    "        x = layer(x)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Get attention weights\n",
    "sample_input = torch.randint(0, vocab_size, (1, 8))\n",
    "attn_weights = get_attention_weights(encoder, sample_input, layer_idx=0)\n",
    "\n",
    "# Visualize all heads\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    attn = attn_weights[0, head_idx].detach().numpy()\n",
    "    sns.heatmap(attn, ax=axes[head_idx], cmap='YlOrRd', cbar=False, square=True)\n",
    "    axes[head_idx].set_title(f'Head {head_idx + 1}', fontsize=10)\n",
    "    axes[head_idx].set_xlabel('Key')\n",
    "    axes[head_idx].set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Attention Patterns Across All Heads (Layer 1)', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Components of Transformer Block:\n",
    "\n",
    "1. **Multi-Head Attention**: Captures different types of relationships\n",
    "2. **Feed-Forward Network**: Processes each position independently\n",
    "3. **Layer Normalization**: Stabilizes training\n",
    "4. **Residual Connections**: Enables deep networks\n",
    "5. **Positional Encoding**: Adds sequence order information\n",
    "\n",
    "### Why This Architecture Works:\n",
    "\n",
    "- **Parallel Processing**: All positions computed simultaneously\n",
    "- **Long-Range Dependencies**: Direct connections between any two positions\n",
    "- **Flexible Attention**: Can learn various linguistic patterns\n",
    "- **Stable Training**: Residuals and normalization enable deep stacking\n",
    "\n",
    "This forms the foundation for models like BERT and GPT!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
