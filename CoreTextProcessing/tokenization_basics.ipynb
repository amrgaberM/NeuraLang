{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc4a24f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root folder to sys.path\n",
    "project_root = os.path.abspath(\"..\")  # parent folder of notebooks\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af339608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_preprocessing import clean_text,tokenize\n",
    "from data.corpus import raw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d35d463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. this is the first sentence\n",
      "2. here is another with punctuation\n",
      "3. and a third one just to be sure\n",
      "4. email me at email for updates\n",
      "5. numbers like num or num should be handled\n",
      "6. special symbols and appear often\n",
      "7. mixed case words and extra spaces here\n",
      "8. short contractions like cant wont and its\n",
      "9. multiple sentences yes right here\n",
      "10. tabs and newlines are also common\n",
      "11. sometimes emojis appear emojiemojiemoji in text\n",
      "12. hyphenatedwords and underscores are frequent\n",
      "13. quotes single and double marks appear\n",
      "14. urls like url or url are common\n",
      "15. end of corpus let us test cleaning thoroughly\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus=[clean_text(i) for i in raw_corpus ]\n",
    "for i, sentence in enumerate(cleaned_corpus, 1):\n",
    "    print(f\"{i}. {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b2d40c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ['this', 'is', 'the', 'first', 'sentence']\n",
      "2. ['here', 'is', 'another', 'with', 'punctuation']\n",
      "3. ['and', 'a', 'third', 'one', 'just', 'to', 'be', 'sure']\n",
      "4. ['email', 'me', 'at', 'email', 'for', 'updates']\n",
      "5. ['numbers', 'like', 'num', 'or', 'num', 'should', 'be', 'handled']\n",
      "6. ['special', 'symbols', 'and', 'appear', 'often']\n",
      "7. ['mixed', 'case', 'words', 'and', 'extra', 'spaces', 'here']\n",
      "8. ['short', 'contractions', 'like', 'cant', 'wont', 'and', 'its']\n",
      "9. ['multiple', 'sentences', 'yes', 'right', 'here']\n",
      "10. ['tabs', 'and', 'newlines', 'are', 'also', 'common']\n",
      "11. ['sometimes', 'emojis', 'appear', 'emojiemojiemoji', 'in', 'text']\n",
      "12. ['hyphenatedwords', 'and', 'underscores', 'are', 'frequent']\n",
      "13. ['quotes', 'single', 'and', 'double', 'marks', 'appear']\n",
      "14. ['urls', 'like', 'url', 'or', 'url', 'are', 'common']\n",
      "15. ['end', 'of', 'corpus', 'let', 'us', 'test', 'cleaning', 'thoroughly']\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = [tokenize(sentence) for sentence in cleaned_corpus]\n",
    "\n",
    "for i, tokens in enumerate(tokenized_corpus, 1):\n",
    "    print(f\"{i}. {tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
